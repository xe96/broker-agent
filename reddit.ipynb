{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xe96/broker-agent/blob/kai-dev/reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Config"
      ],
      "metadata": {
        "id": "Wxlrj5632YKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "688A3_8Tr53c",
        "outputId": "6eb7e3ea-f036-4a45-d3df-8bd86510b4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.12/dist-packages (7.8.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.12/dist-packages (3.3.2)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.12/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.12/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.8.3)\n",
            "Collecting supabase\n",
            "  Downloading supabase-2.18.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting postgrest==1.1.1 (from supabase)\n",
            "  Downloading postgrest-1.1.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting realtime==2.7.0 (from supabase)\n",
            "  Downloading realtime-2.7.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting supabase-auth==2.12.3 (from supabase)\n",
            "  Downloading supabase_auth-2.12.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting storage3==0.12.1 (from supabase)\n",
            "  Downloading storage3-0.12.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting supabase-functions==0.10.1 (from supabase)\n",
            "  Downloading supabase_functions-0.10.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.12/dist-packages (from supabase) (0.28.1)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest==1.1.1->supabase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.9 in /usr/local/lib/python3.12/dist-packages (from postgrest==1.1.1->supabase) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from realtime==2.7.0->supabase) (4.15.0)\n",
            "Requirement already satisfied: websockets<16,>=11 in /usr/local/lib/python3.12/dist-packages (from realtime==2.7.0->supabase) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from storage3==0.12.1->supabase) (2.9.0.post0)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from supabase-auth==2.12.3->supabase) (2.10.1)\n",
            "Collecting strenum<0.5.0,>=0.4.15 (from supabase-functions==0.10.1->supabase)\n",
            "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation<3.0.0,>=2.1.0->postgrest==1.1.1->supabase) (25.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<0.29,>=0.26->postgrest==1.1.1->supabase) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest==1.1.1->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest==1.1.1->supabase) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest==1.1.1->supabase) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->storage3==0.12.1->supabase) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==1.1.1->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==1.1.1->supabase) (4.1.0)\n",
            "Downloading supabase-2.18.1-py3-none-any.whl (18 kB)\n",
            "Downloading postgrest-1.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading realtime-2.7.0-py3-none-any.whl (22 kB)\n",
            "Downloading storage3-0.12.1-py3-none-any.whl (18 kB)\n",
            "Downloading supabase_auth-2.12.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading supabase_functions-0.10.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
            "Installing collected packages: strenum, deprecation, realtime, supabase-functions, supabase-auth, storage3, postgrest, supabase\n",
            "Successfully installed deprecation-2.1.0 postgrest-1.1.1 realtime-2.7.0 storage3-0.12.1 strenum-0.4.15 supabase-2.18.1 supabase-auth-2.12.3 supabase-functions-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tweepy praw python-dotenv vaderSentiment\n",
        "!pip install supabase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import praw\n",
        "from supabase import create_client\n",
        "import datetime as dt"
      ],
      "metadata": {
        "id": "B5_8fzydzYz6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"REDDIT_CLIENT_ID\"] = \"w8vAFuTjHB6TWiaB-5muLw\"\n",
        "os.environ[\"REDDIT_CLIENT_SECRET\"] = \"Wlalndnv58IvLGef453qYI8JUVT0iw\"\n",
        "os.environ[\"REDDIT_USER_AGENT\"]='sentiment-demo/0.1 by ivan'"
      ],
      "metadata": {
        "id": "d4jA2Q3quMbK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SUPABASE_URL\"] = \"https://cflxvisjojofzuzyptxv.supabase.co\"\n",
        "os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImNmbHh2aXNqb2pvZnp1enlwdHh2Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NjQwMzI5MywiZXhwIjoyMDcxOTc5MjkzfQ.VrV4lBSQAix2O-OZbABFIfH_mEhGTtO9CPbbEJN1kGs\""
      ],
      "metadata": {
        "id": "zMFYUwMq0Oj_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit Crawler"
      ],
      "metadata": {
        "id": "ZuEuiLr42bBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(\n",
        "    client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
        "    client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
        "    user_agent=os.environ[\"REDDIT_USER_AGENT\"],\n",
        ")\n",
        "reddit.read_only = True\n",
        "\n",
        "# 3) Sanity check: a very simple call\n",
        "try:\n",
        "    print(\"Read-only:\", reddit.read_only)\n",
        "    it = reddit.subreddit(\"python\").hot(limit=1)\n",
        "    print(next(it).title)\n",
        "    print(\"OK ✅\")\n",
        "except Exception as e:\n",
        "    print(\"Failed ❌ ->\", type(e).__name__, e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF83h0qRsFHo",
        "outputId": "3f2f8962-fbb7-47de-c705-3ad47d269f08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read-only: True\n",
            "Sunday Daily Thread: What's everyone working on this week?\n",
            "OK ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub = reddit.subreddit(\"stocks\")   # choose any subreddit\n",
        "\n",
        "# Top N new posts matching a keyword\n",
        "results = []\n",
        "for post in sub.search(query=\"nvidia\", sort=\"new\", limit=25):\n",
        "    results.append({\n",
        "        \"id\": post.id,\n",
        "        \"title\": post.title,\n",
        "        \"score\": post.score,\n",
        "        \"created_utc\": post.created_utc,\n",
        "        \"url\": post.url,\n",
        "        \"num_comments\": post.num_comments,\n",
        "        \"selftext\": post.selftext[:500],  # preview\n",
        "    })\n",
        "\n",
        "print(f\"Fetched {len(results)} posts\")\n",
        "print(results[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS28Z3bRuoha",
        "outputId": "63932bed-1b1b-40e6-e76b-84bdd936b944"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 25 posts\n",
            "[{'id': '1n1y5cm', 'title': 'Nvidia CEO Huang says bringing Blackwell AI chip to China ‘is a real possibility’', 'score': 289, 'created_utc': 1756341447.0, 'url': 'https://www.reddit.com/r/stocks/comments/1n1y5cm/nvidia_ceo_huang_says_bringing_blackwell_ai_chip/', 'num_comments': 67, 'selftext': 'Nvidia\\xa0CEO Jensen Huang said there’s a “real possibility” the company brings its advanced Blackwell processor to China as he urges the U.S. government to open up access for American chipmakers.\\n\\nHe also predicted the artificial intelligence market in the world’s second-biggest economy will grow 50% next year.\\n\\n“The opportunity for us to bring Blackwell to the China market is a real possibility,” Huang said on Wednesday in a call for Nvidia’s latest quarterly\\xa0results. “We just have to keep advoca'}, {'id': '1n1s58d', 'title': 'NVDA 2Q26 Earnings Result', 'score': 465, 'created_utc': 1756326398.0, 'url': 'https://www.reddit.com/r/stocks/comments/1n1s58d/nvda_2q26_earnings_result/', 'num_comments': 193, 'selftext': '**Earnings per share**: $1.05, adjusted, up 54% from a year ago\\n\\n**Revenue**: Revenue of $46.7 billion, up 6% from Q1 and up 56% from a year ago\\n\\nNVIDIA (NASDAQ: NVDA) today reported revenue for the second quarter ended July 27, 2025, of $46.7 billion, up 6% from the previous quarter and up 56% from a year ago. NVIDIA’s Blackwell Data Center revenue grew 17% sequentially.\\n\\nThere were no H20 sales to China-based customers in the second quarter.\\xa0NVIDIA benefited from a $180 million release of prev'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if results:\n",
        "    post = reddit.submission(id=results[0][\"id\"])\n",
        "    post.comments.replace_more(limit=0)  # flatten \"MoreComments\"\n",
        "    comments = [c.body for c in post.comments.list()[:50]]  # first 50\n",
        "    print(f\"Collected {len(comments)} comments\")\n",
        "    print(comments[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSWsM58mvuLP",
        "outputId": "99d55d63-b881-407b-c285-35f8fda185e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 50 comments\n",
            "['Hi, you\\'re on r/Stocks, please make sure your post is related to stocks or the stockmarket or it will most likely get removed as being off-topic/political; feel free to edit it now and be more specific.\\n\\n**To everyone commenting:**  Please focus on how this affects the stock market or specific stocks or it will be removed as being off-topic/political.\\n\\nIf you\\'re interested in just politics, see our wiki on [\"relevant subreddits\"](https://www.reddit.com/r/stocks/wiki/index/#wiki_relevant_subreddits) and post to those Reddit communities instead without linking back here, thanks!\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/stocks) if you have any questions or concerns.*', 'If Trump had any brains he would understand that we WANT Chinese to become reliant and dependent on our technology.\\xa0', 'guess how much of sales the 47th is gonna ask this time']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def score(text):\n",
        "    s = analyzer.polarity_scores(text)\n",
        "    return s[\"compound\"]  # -1 (neg) .. +1 (pos)\n",
        "\n",
        "# Example: score tweets\n",
        "if 'all_rows' in globals() and all_rows:\n",
        "    for r in all_rows[:10]:\n",
        "        r[\"sentiment\"] = score(r[\"text\"])\n",
        "    print(all_rows[:3])\n",
        "\n",
        "# Example: score reddit comments\n",
        "if 'comments' in globals() and comments:\n",
        "    scored = [{\"text\": c[:120]+\"...\", \"sentiment\": score(c)} for c in comments[:10]]\n",
        "    print(scored[:3])"
      ],
      "metadata": {
        "id": "Zo1PGd7tvyaE",
        "outputId": "d6dac154-b87b-4aa3-a19c-971bb73d11f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': \"Hi, you're on r/Stocks, please make sure your post is related to stocks or the stockmarket or it will most likely get re...\", 'sentiment': 0.9468}, {'text': 'If Trump had any brains he would understand that we WANT Chinese to become reliant and dependent on our technology.\\xa0...', 'sentiment': 0.368}, {'text': 'guess how much of sales the 47th is gonna ask this time...', 'sentiment': 0.0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supabase Writer"
      ],
      "metadata": {
        "id": "_KQABq2j2eMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supabase = create_client(os.getenv(\"SUPABASE_URL\"), os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\"))"
      ],
      "metadata": {
        "id": "AXwbe_mDv1SM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_reddit_item(post, query:str, tickers:list[str], sentiment:float):\n",
        "    \"\"\"\n",
        "    post: PRAW 的 Submission 或 Comment 对象\n",
        "    query: 搜索时的关键词，例如 \"TSLA OR $TSLA\"\n",
        "    tickers: 抽取出的股票代码列表\n",
        "    sentiment: 情绪分数（-1 到 1）\n",
        "    \"\"\"\n",
        "    # 算互动速度\n",
        "    age_h = max(\n",
        "        (dt.datetime.utcnow() - dt.datetime.utcfromtimestamp(post.created_utc)).total_seconds()/3600,\n",
        "        0.1\n",
        "    )\n",
        "    engagement_velocity = (getattr(post, \"score\", 0) or 0) / age_h\n",
        "\n",
        "    row = {\n",
        "        \"id\": post.id,\n",
        "        \"kind\": \"post\" if hasattr(post, \"title\") else \"comment\",\n",
        "        \"subreddit\": str(post.subreddit),\n",
        "        \"author\": str(post.author) if post.author else None,\n",
        "        \"title\": getattr(post, \"title\", None),\n",
        "        \"body\": getattr(post, \"selftext\", getattr(post, \"body\", None)),\n",
        "        \"url\": getattr(post, \"url\", None),\n",
        "        \"permalink\": f\"https://reddit.com{post.permalink}\",\n",
        "        \"score\": getattr(post, \"score\", None),\n",
        "        \"num_comments\": getattr(post, \"num_comments\", None),\n",
        "        \"upvote_ratio\": getattr(post, \"upvote_ratio\", None),\n",
        "        \"awards\": getattr(post, \"all_awardings\", None),\n",
        "        \"created_utc\": dt.datetime.utcfromtimestamp(post.created_utc).isoformat()+\"Z\",\n",
        "        \"query\": query,\n",
        "        \"tickers\": tickers,\n",
        "        \"sentiment\": sentiment,\n",
        "        \"engagement_velocity\": engagement_velocity\n",
        "    }\n",
        "\n",
        "    # upsert，防止重复\n",
        "    resp = supabase.table(\"reddit_items\").upsert(row, on_conflict=\"id\").execute()\n",
        "    return resp\n"
      ],
      "metadata": {
        "id": "uHNsu8nD2MyY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 例子：从搜索结果里存一条数据\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 取一条帖子\n",
        "sub = reddit.subreddit(\"wallstreetbets\")\n",
        "post = next(sub.search(\"TSLA\", sort=\"new\", limit=1))\n",
        "\n",
        "# 情绪打分\n",
        "s = analyzer.polarity_scores(post.title + \" \" + (post.selftext or \"\"))[\"compound\"]\n",
        "\n",
        "# 存进数据库\n",
        "resp = insert_reddit_item(post, query=\"TSLA\", tickers=[\"TSLA\"], sentiment=s)\n",
        "print(resp)\n"
      ],
      "metadata": {
        "id": "yO9PolCH2gFS",
        "outputId": "7d470600-0c5b-4273-cacb-3f1eec5927f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "/tmp/ipython-input-3651136790.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  (dt.datetime.utcnow() - dt.datetime.utcfromtimestamp(post.created_utc)).total_seconds()/3600,\n",
            "/tmp/ipython-input-3651136790.py:10: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
            "  (dt.datetime.utcnow() - dt.datetime.utcfromtimestamp(post.created_utc)).total_seconds()/3600,\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "/tmp/ipython-input-3651136790.py:28: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
            "  \"created_utc\": dt.datetime.utcfromtimestamp(post.created_utc).isoformat()+\"Z\",\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data=[{'id': '1mzxcdv', 'kind': 'post', 'subreddit': 'wallstreetbets', 'author': 'w00tsick', 'title': 'TSLA Short YOLO incl retirement', 'body': 'How regarded is this play? Opened today \\\\~345 levels\\n\\nEDIT: Cooked\\n\\nEDIT EDIT: Maybe not cooked?\\n\\nEDIT EDIT EDIT: Closed for a few hundred loss around $340.50, may reopen for longer dates on a cat bounce.', 'url': 'https://i.redd.it/2dnoernyc7lf1.png', 'permalink': 'https://reddit.com/r/wallstreetbets/comments/1mzxcdv/tsla_short_yolo_incl_retirement/', 'score': 29, 'num_comments': 38, 'upvote_ratio': 0.77, 'awards': [], 'created_utc': '2025-08-25T17:45:10+00:00', 'retrieved_at': '2025-08-28T18:04:37.12453+00:00', 'query': 'TSLA', 'tickers': ['TSLA'], 'sentiment': 0.5637, 'engagement_velocity': 0.45628125019236576}] count=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vLIyhNR42lj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}